# üîç Empirical Comparison of ChatGPT-o3 and DeepSeek-R1 for Code Generation

## üìå Overview

This project presents a comparative study of two advanced large language models‚Äî**ChatGPT-o3 (GPT-3.5-turbo-instruct)** and **DeepSeek-Coder R1**‚Äîfor backend **Java Spring Boot** microservice code generation. Our goal was to evaluate the quality, maintainability, and architectural soundness of code generated by both models under identical prompt conditions.

The code simulates the backend for a LeetCode-style coding platform, broken into four microservices: **AuthService**, **ProfileService**, **QuestionService**, and **CodeService**.

---

## üéØ Objectives

- Compare ChatGPT-o3 and DeepSeek-R1 in terms of:
  - Code quality (cyclomatic complexity, cognitive complexity, code smells, vulnerabilities)
  - Architectural principles (modularity, REST adherence, layering)
  - Prompt efficiency (number of iterations needed)
- Propose use-case-based model recommendations
- Contribute novel normalized complexity metrics per method

---

## üß± Microservices

| Service         | Responsibility                                |
|-----------------|-----------------------------------------------|
| `AuthService`   | User login, registration, and JWT generation  |
| `ProfileService`| User profile CRUD operations                  |
| `QuestionService`| Coding question management and filtering     |
| `CodeService`   | Code submission and execution handling        |

Each microservice was generated **independently** by both models using consistent prompts, then evaluated.

---

## ‚öôÔ∏è Tech Stack

- **Language**: Java 17  
- **Framework**: Spring Boot 3.x  
- **Build Tool**: Maven 3.4.4  
- **Databases**: MongoDB / MySQL (as chosen by the models)  
- **Static Analysis**: SonarQube (Community Edition)  
- **IDE**: IntelliJ IDEA  
- **Version Control**: GitHub  

---

## üìä Evaluation Framework

### Prompt Efficiency
- Prompt counts were tracked for each model per microservice.
- Fewer prompts = better contextual understanding.

### Code Quality Metrics (via SonarQube)
- **Cyclomatic Complexity**
- **Cognitive Complexity**
- **Code Smells**
- **Security Vulnerabilities**

### Derived Metrics (Novel)
- `Average Cyclomatic Complexity per Method`
- `Average Cognitive Complexity per Method`

### Manual Architecture Review
- REST conventions
- Separation of controller/service/repository layers
- DTO usage
- Externalized configuration

---

## üìà Key Findings

- **ChatGPT-o3** produced more readable code (lower cognitive complexity).
- **DeepSeek-R1** generated code with cleaner structure and fewer code smells.
- Both models showed comparable results in vulnerability detection and architectural adherence.

### When to Use What?

| If you value...                    | Recommended Model |
|-----------------------------------|-------------------|
| Readability and expressive logic  | ChatGPT-o3        |
| Concise, clean, maintainable code | DeepSeek-R1       |

---

## üî≠ Future Work

- Automate unit and integration test case generation
- Deploy services with Docker and test end-to-end flow
- Extend evaluation to include frontend code generation
- Build structured error taxonomy for model failures




